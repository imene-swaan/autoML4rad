{
    "page-blocks": {
        "auc_score": 0.9490593112244898,
        "params": {
            "bag": 5,
            "lr": 0.03900328485834744,
            "depth": 2,
            "batch": 50,
            "rounds": 38
        },
        "evaluation_cost": 153.46108651161194
    },
    "qsar-biodeg": {
        "auc_score": 0.8046457814168744,
        "params": {
            "bag": 15,
            "lr": 0.04257417833128574,
            "depth": 2,
            "batch": 56,
            "rounds": 37
        },
        "evaluation_cost": 116.8054587841034
    },
    "waveform-5000": {
        "auc_score": 0.7979801516442391,
        "params": {
            "bag": 14,
            "lr": 0.04257417833128574,
            "depth": 2,
            "batch": 56,
            "rounds": 37
        },
        "evaluation_cost": 54.84820294380188
    },
    "gas-drift": {
        "auc_score": 0.9906977294748892,
        "params": {
            "bag": 98,
            "lr": 0.03966940937065277,
            "depth": 2,
            "batch": 50,
            "rounds": 38
        },
        "evaluation_cost": 125.39555501937866
    },
    "hill-valley": {
        "auc_score": 0.5248924025518147,
        "params": {
            "bag": 59,
            "lr": 0.03941798243996576,
            "depth": 2,
            "batch": 65,
            "rounds": 34
        },
        "evaluation_cost": 54.00697660446167
    },
    "shuttle": {
        "auc_score": 0.9934682234400068,
        "params": {
            "bag": 4,
            "lr": 0.04257417833128574,
            "depth": 2,
            "batch": 71,
            "rounds": 37
        },
        "evaluation_cost": 327.5998101234436
    },
    "HeartC": {
        "auc_score": 0.6049970255800119,
        "params": {
            "bag": 7,
            "lr": 0.03941798243996576,
            "depth": 2,
            "batch": 50,
            "rounds": 34
        },
        "evaluation_cost": 43.835049629211426
    },
    "Vowel_0": {
        "auc_score": 0.9591836734693877,
        "params": {
            "bag": 5,
            "lr": 0.03941798243996576,
            "depth": 2,
            "batch": 65,
            "rounds": 34
        },
        "evaluation_cost": 57.396326780319214
    },
    "ozone-level-8hr": {
        "auc_score": 0.857204861111111,
        "params": {
            "bag": 42,
            "lr": 0.03900328485834744,
            "depth": 2,
            "batch": 50,
            "rounds": 38
        },
        "evaluation_cost": 150.21240997314453
    },
    "Bioresponse": {
        "auc_score": 0.5811233539371137,
        "params": {
            "bag": 743,
            "lr": 0.03916655550927876,
            "depth": 2,
            "batch": 77,
            "rounds": 30
        },
        "evaluation_cost": 103.29233598709106
    },
    "mnist": {
        "auc_score": 0.9165734693877551,
        "params": {
            "bag": 46,
            "lr": 0.03941798243996576,
            "depth": 2,
            "batch": 65,
            "rounds": 34
        },
        "evaluation_cost": 105.3506531715393
    },
    "Abalone_1_8": {
        "auc_score": 0.8191920666651693,
        "params": {
            "bag": 6,
            "lr": 0.040498804533889404,
            "depth": 2,
            "batch": 53,
            "rounds": 30
        },
        "evaluation_cost": 67.77926754951477
    },
    "Concordia0": {
        "auc_score": 0.9753472222222221,
        "params": {
            "bag": 535,
            "lr": 0.042825605261972745,
            "depth": 2,
            "batch": 50,
            "rounds": 41
        },
        "evaluation_cost": 251.99888825416565
    },
    "Concordia3_32": {
        "auc_score": 0.8784027777777779,
        "params": {
            "bag": 134,
            "lr": 0.042825605261972745,
            "depth": 2,
            "batch": 50,
            "rounds": 41
        },
        "evaluation_cost": 168.60655045509338
    },
    "Delft_pump_5x3_noisy": {
        "auc_score": 0.63427734375,
        "params": {
            "bag": 26,
            "lr": 0.038337160346042114,
            "depth": 2,
            "batch": 62,
            "rounds": 38
        },
        "evaluation_cost": 62.755574226379395
    },
    "Delft_pump_AR": {
        "auc_score": 0.9824617346938775,
        "params": {
            "bag": 43,
            "lr": 0.03987398996796619,
            "depth": 2,
            "batch": 50,
            "rounds": 37
        },
        "evaluation_cost": 480.8647177219391
    },
    "Ecoli": {
        "auc_score": 0.9955555555555555,
        "params": {
            "bag": 3,
            "lr": 0.03900328485834744,
            "depth": 2,
            "batch": 50,
            "rounds": 38
        },
        "evaluation_cost": 117.080246925354
    },
    "Glass_building_float": {
        "auc_score": 0.6666666666666666,
        "params": {
            "bag": 4,
            "lr": 0.035299916021681185,
            "depth": 2,
            "batch": 50,
            "rounds": 35
        },
        "evaluation_cost": 149.27695727348328
    },
    "Housing_low": {
        "auc_score": 0.8702256944444444,
        "params": {
            "bag": 6,
            "lr": 0.043240302843591066,
            "depth": 2,
            "batch": 59,
            "rounds": 37
        },
        "evaluation_cost": 257.8977589607239
    },
    "MagicTelescope": {
        "auc_score": 0.814303143093919,
        "params": {
            "bag": 7,
            "lr": 0.04008410695227109,
            "depth": 2,
            "batch": 53,
            "rounds": 34
        },
        "evaluation_cost": 262.407705783844
    },
    "Sonar_mines": {
        "auc_score": 0.6528925619834711,
        "params": {
            "bag": 43,
            "lr": 0.04349172977427807,
            "depth": 2,
            "batch": 50,
            "rounds": 41
        },
        "evaluation_cost": 532.5233597755432
    },
    "Spectf_0": {
        "auc_score": 0.7589285714285714,
        "params": {
            "bag": 28,
            "lr": 0.03559566203634046,
            "depth": 2,
            "batch": 56,
            "rounds": 31
        },
        "evaluation_cost": 33.70389652252197
    },
    "Vehicle_van": {
        "auc_score": 0.8638322321172076,
        "params": {
            "bag": 10,
            "lr": 0.03941798243996576,
            "depth": 2,
            "batch": 65,
            "rounds": 34
        },
        "evaluation_cost": 207.72063517570496
    },
    "annthyroid": {
        "auc_score": 0.8718525999803617,
        "params": {
            "bag": 2,
            "lr": 0.04257417833128574,
            "depth": 2,
            "batch": 56,
            "rounds": 37
        },
        "evaluation_cost": 372.77444338798523
    },
    "cardio": {
        "auc_score": 0.9626807851239669,
        "params": {
            "bag": 11,
            "lr": 0.042825605261972745,
            "depth": 2,
            "batch": 50,
            "rounds": 41
        },
        "evaluation_cost": 72.6232430934906
    },
    "elevators": {
        "auc_score": 0.7796594442942131,
        "params": {
            "bag": 9,
            "lr": 0.03713249165826453,
            "depth": 2,
            "batch": 50,
            "rounds": 30
        },
        "evaluation_cost": 284.8921847343445
    },
    "ionosphere": {
        "auc_score": 0.9930942303408331,
        "params": {
            "bag": 13,
            "lr": 0.038337160346042114,
            "depth": 2,
            "batch": 62,
            "rounds": 38
        },
        "evaluation_cost": 228.15236234664917
    },
    "letter": {
        "auc_score": 0.7373,
        "params": {
            "bag": 16,
            "lr": 0.04061775610084846,
            "depth": 2,
            "batch": 50,
            "rounds": 30
        },
        "evaluation_cost": 117.5901346206665
    },
    "mammography": {
        "auc_score": 0.9076997041420118,
        "params": {
            "bag": 2,
            "lr": 0.038337160346042114,
            "depth": 2,
            "batch": 62,
            "rounds": 38
        },
        "evaluation_cost": 434.2078483104706
    },
    "mnist7": {
        "auc_score": 0.9317249314902574,
        "params": {
            "bag": 369,
            "lr": 0.03762972588735468,
            "depth": 2,
            "batch": 98,
            "rounds": 31
        },
        "evaluation_cost": 150.4963607788086
    },
    "musk": {
        "auc_score": 1.0,
        "params": {
            "bag": 98,
            "lr": 0.03941798243996576,
            "depth": 2,
            "batch": 50,
            "rounds": 34
        },
        "evaluation_cost": 231.39290690422058
    },
    "pc3": {
        "auc_score": 0.71322265625,
        "params": {
            "bag": 13,
            "lr": 0.04215948074966742,
            "depth": 2,
            "batch": 56,
            "rounds": 41
        },
        "evaluation_cost": 88.2309582233429
    },
    "pendigits": {
        "auc_score": 0.9038050624589087,
        "params": {
            "bag": 7,
            "lr": 0.03916655550927876,
            "depth": 2,
            "batch": 77,
            "rounds": 30
        },
        "evaluation_cost": 77.03020286560059
    },
    "satellite": {
        "auc_score": 0.8237602751182489,
        "params": {
            "bag": 14,
            "lr": 0.032229349160715584,
            "depth": 2,
            "batch": 53,
            "rounds": 31
        },
        "evaluation_cost": 148.416677236557
    },
    "segment": {
        "auc_score": 1.0,
        "params": {
            "bag": 10,
            "lr": 0.03941798243996576,
            "depth": 2,
            "batch": 50,
            "rounds": 34
        },
        "evaluation_cost": 982.0478477478027
    },
    "spambase": {
        "auc_score": 0.8810352327098739,
        "params": {
            "bag": 16,
            "lr": 0.04028868754958451,
            "depth": 2,
            "batch": 50,
            "rounds": 33
        },
        "evaluation_cost": 88.08013820648193
    },
    "speech": {
        "auc_score": 0.6277882289707067,
        "params": {
            "bag": 267,
            "lr": 0.038632906360701394,
            "depth": 2,
            "batch": 85,
            "rounds": 34
        },
        "evaluation_cost": 72.06518483161926
    },
    "thyroid": {
        "auc_score": 0.9883223494045554,
        "params": {
            "bag": 3,
            "lr": 0.035225283538694405,
            "depth": 2,
            "batch": 91,
            "rounds": 27
        },
        "evaluation_cost": 194.62404322624207
    },
    "vertebral": {
        "auc_score": 0.6522222222222223,
        "params": {
            "bag": 3,
            "lr": 0.038632906360701394,
            "depth": 2,
            "batch": 70,
            "rounds": 34
        },
        "evaluation_cost": 56.93872261047363
    },
    "vowels": {
        "auc_score": 0.9888,
        "params": {
            "bag": 10,
            "lr": 0.03547671046938141,
            "depth": 2,
            "batch": 76,
            "rounds": 31
        },
        "evaluation_cost": 164.96852707862854
    }
}